{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371351a6-5df9-4d94-892b-53e81f9c64ca",
   "metadata": {},
   "source": [
    "# HR Analytics: Predicting Employee Churn in Python\n",
    "\n",
    "Among all of the business domains, HR is still the least disrupted. However, the latest developments in data collection and analysis tools and technologies allow for data driven decision-making in all dimensions, including HR. This course will provide a solid basis for dealing with employee data and developing a predictive model to analyze employee turnover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570922b-ff19-4865-96f2-4d1e6cb5cef3",
   "metadata": {},
   "source": [
    "## A. Introduction to HR Analytics\n",
    "\n",
    "In this chapter you will learn about the problems addressed by HR analytics, as well as will explore a sample HR dataset that will further be analyzed. You will describe and visualize some of the key variables, transform and manipulate the dataset to make it ready for analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d73674-955a-4271-9ff6-a3a66b0e8754",
   "metadata": {},
   "source": [
    "### 1. Introduction to HR analytics\n",
    "\n",
    "Hello and welcome to \"HR analytics in Python\" course. My name is Hrant Davtyan, I am a Business Analyst teaching Data Science and providing consultancy related to statistics. Among all of the business domains, HR is still the least disrupted. However, the latest developments in Data collection and analysis tools and technologies allow for data driven decision-making in all dimensions including HR. As a consequence, HR analytics is a growing field and I believe it is the correct time to tap into that industry.\n",
    "\n",
    "2. What is HR analytics?\n",
    "00:34 - 00:44\n",
    "HR analytics is also known as People analytics and it is nothing else than a data-driven approach to managing people at work.\n",
    "\n",
    "3. Problems addressed by HR analytics\n",
    "00:44 - 01:08\n",
    "There are many problems in HR that can be addressed using data-driven approach. Among those are decisions related to employee hiring and retention, performance evaluation, collaboration and else. In this course, we will concentrate on Predicting employee turnover which is related to the first 2 bullet points: Hiring and retention.\n",
    "\n",
    "4. Employee turnover\n",
    "01:08 - 01:48\n",
    "Employee turnover is the process of employees leaving the company also known as employee attrition or employee churn. When skilled employees leave, this can be very costly for the company, thus firms are interested in predicting turnover beforehand. Having that information in hand, companies can change their strategy to retain good workers or start the hiring process of new employees on time.\n",
    "\n",
    "5. Course structure\n",
    "01:48 - 02:11\n",
    "In this course, we will use a sample employee dataset with variables that describe employees in the company to predict their turnover and understand what are the most important features affecting it. The 1st chapter will concentrate on descriptive analytics, where we will transform the dataset and make it ready for developing the predictive model. In the 2nd chapter we will develop an initial model that will then be tuned and improved in the 3rd chapter. The final chapter will introduce techniques that will allow selection of the best model for decision-making.\n",
    "\n",
    "6. The dataset\n",
    "02:11 - 03:15\n",
    "So let's start by taking a quick look to our dataset. The data is provided in csv format and is located in the working directory. This means we can use the read_csv() function from the pandas library to read it. Once the dataset is read into a new pandas DataFrame called \"data\", we can use the info() method to get some information on it. As you can see from the output we have 10 columns and almost 15000 entries, which means the DataFrame includes data on almost 15000 employees about 10 different variables. Among those 10, only 2 have the type object, while others are either float or int. The latter means that our variables are numeric, numbers, that can be used to perform mathematical and statistical computations on, while the object types are called categorical variables and they need to be transformed first, before moving on. Therefore, let's take a quick look to our dataset to see what it looks like and what are those 2 categorical variables we have there.\n",
    "\n",
    "7. The dataset\n",
    "03:15 - 03:44\n",
    "We can use the head() method to take a look to the first 5 rows of the DataFrame. As you can see, the last two columns are \"department\" and \"salary\", which are giving information about the department an employee is working at and the salary s/he is receiving, respectively. Both of them describe some category of an employee (belonging to this or that department or salary group), which is the reason they are called categorical variables.\n",
    "\n",
    "8. Unique values\n",
    "03:44 - 03:55\n",
    "In order to understand what are the values that those columns get, we have to first choose the relevant columns, and then use a method called **unique()** to print only the unique values in that column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c3033-29ae-49b0-a3a3-8637fcf381dc",
   "metadata": {},
   "source": [
    "**Finding categorical variables**\n",
    "\n",
    "Categorical variables are variables that receive a limited number of values that describe a category. They can be of two types:\n",
    "\n",
    "Ordinal – variables with two or more categories that can be ranked or ordered (e.g. “low”, “medium”, “high”)\n",
    "Nominal – variables with two or more categories that do not have an intrinsic order (e.g. “men”, “women”)\n",
    "In this exercise, you will find the categorical variables in the dataset. To do that, first of all, you will import the pandas library and read the CSV file called \"turnover.csv\". Then, after viewing the first 5 rows and learning (visually) that there are non-numeric values in the DataFrame, you will get some information about the types of variables that are available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ce272-2187-40e8-8084-53bf29611a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas (as pd) to read the data\n",
    "import pandas as pd\n",
    "\n",
    "# Read \"turnover.csv\" and save it in a DataFrame called data\n",
    "data = pd.read_csv(\"turnover.csv\")\n",
    "\n",
    "# Take a quick look to the first 5 rows of data\n",
    "print(data.head(5))\n",
    "\n",
    "# Get some information on the types of variables in data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1ca95-856c-4284-adc3-b772c4c608ae",
   "metadata": {},
   "source": [
    "**Observing categoricals**\n",
    "\n",
    "Remember from the previous exercise that:\n",
    "\n",
    "Ordinal variables have two or more categories which can be ranked or ordered\n",
    "Nominal variables have two or more categories which do not have an intrinsic order\n",
    "In your dataset:\n",
    "\n",
    "salary is an ordinal variable\n",
    "department is a nominal variable\n",
    "In this exercise, you're going to observe the categorical variables found in the previous exercise. To do that, first of all, you will import the pandas library and read the CSV file called \"turnover.csv\". Then, you will print the unique values of those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999aae9-b9c4-43ef-b8e7-5b021a633db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas (as pd) to read the data\n",
    "import pandas as pd\n",
    "\n",
    "# Read \"turnover.csv\" file and save it in a DataFrame called data\n",
    "data = pd.read_csv(\"turnover.csv\")\n",
    "\n",
    "# Print the unique values of the \"department\" column\n",
    "print(data.department.unique())\n",
    "\n",
    "# Print the unique values of the \"salary\" column\n",
    "print(data.salary.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7c350-7a3a-4c4b-a655-6986b5cfa51f",
   "metadata": {},
   "source": [
    "### 2. Transforming categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd136b19-d950-4360-ab2e-9f36a9d8afa9",
   "metadata": {},
   "source": [
    "**Encoding categories**\n",
    "\n",
    "You need to help your algorithm understand that you're dealing with categories. You will encode categories of the salary variable, which you know is ordinal based on the values you observed:\n",
    "\n",
    "you first have to tell Python that the salary column is actually categorical\n",
    "you then have to specify the correct order of categories\n",
    "finally, you should encode each category with a numeric value corresponding to its specific position in the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ee72c-f1e2-45d5-b918-c64d48d3f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the type of the \"salary\" column to categorical\n",
    "data.salary = data.salary.astype('category')\n",
    "\n",
    "# Provide the correct order of categories\n",
    "data.salary = data.salary.cat.reorder_categories(['low', 'medium', 'high'])\n",
    "\n",
    "# Encode categories\n",
    "data.salary = data.salary.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb5e9b-35b1-4d30-b79e-b69b7b3edeb3",
   "metadata": {},
   "source": [
    "**Getting dummies**\n",
    "\n",
    "You will now transform the department variable, which you know is nominal based on the values you observed. To do that, you will use so-called dummy variables.\n",
    "\n",
    "`get_dummies()` out of the department column of the data and save them inside a new DataFrame called departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b3c23-28c5-4c99-aa35-0473169858c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies and save them inside a new DataFrame\n",
    "departments = pd.get_dummies(data.department)\n",
    "\n",
    "# Take a quick look to the first 5 rows of the new DataFrame called departments\n",
    "print(departments.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7cf92-8c39-439d-9dcd-e2bde8662661",
   "metadata": {},
   "source": [
    "**Dummy trap**\n",
    "\n",
    "A dummy trap is a situation where different dummy variables convey the same information. In this case, if an employee is, say, from the accounting department (i.e. value in the accounting column is 1), then you're certain that s/he is not from any other department (values everywhere else are 0). Thus, you could actually learn about his/her department by looking at all the other departments.\n",
    "\n",
    "For that reason, whenever \n",
    " dummies are created (in your case, 10), only \n",
    " - 1 (in your case, 9) of them are enough, and the \n",
    "-th column's information is already included.\n",
    "\n",
    "Therefore, you will get rid of the old department column, drop one of the department dummies to avoid dummy trap, and then join the two DataFrames.\n",
    "\n",
    "Instructions\n",
    "\n",
    ".drop() the accounting column to avoid \"dummy trap\".\n",
    ".drop() the old column department as you do not need it anymore.\n",
    "Join the new departments DataFrame to the employee dataset (this has been done for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e901917-e371-4e78-b2e1-5fca922f4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"accounting\" column to avoid \"dummy trap\"\n",
    "departments = departments.drop(\"accounting\", axis=1)\n",
    "\n",
    "# Drop the old column \"department\" as you don't need it anymore\n",
    "data = data.drop(\"department\", axis=1)\n",
    "\n",
    "# Join the new dataframe \"departments\" to your employee dataset: done\n",
    "data = data.join(departments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb32cd4-b6fc-4494-8f83-328586018c78",
   "metadata": {},
   "source": [
    "### 3. Descriptive statistics\n",
    "\n",
    "So now our dataset is ready to develop a predictive algorithm. But before then, let's first get some quick descriptive insights.\n",
    "\n",
    "2. Turnover rate\n",
    "00:07 - 01:06\n",
    "The variable that is providing information whether an employee has left the company or not is the column **churn**. Basically, if the value of this column is 1 then an employee has churned, and if it is 0 then we have not observed turnover in this case. To calculate the turnover rate we have to count number of times this variable has the value 1 and 0 and then divide it by the total. If we multiply the result by 100 then the outcome will be the % of employees who left and stayed. This task is again accomplished in 3 steps: - First we get the number of all the employees, which is basically the length of our data, - Then, we count 1s and 0s in the column churn, - Finally, we divide the counted values by the number of employees and multiple by 100 to get percentages. As you can see around 76% of our employees stayed, while 24% have churned. Thus, we conclude that turnover rate is 24%.\n",
    "\n",
    "3. Correlations\n",
    "01:06 - 01:39\n",
    "Next, we are interested to learn what are the variables that are in a positive or negative linear relationship with our target. To see that, we will first of all develop the correlation matrix using the `corr()` method provided by **pandas** and then visualize the matrix using the `heatmap()` function by seaborn, a statistical visualization library. As you can see the target varaible **churn** has the highest negative correlation with satisfaction level. This shows that the increase in satisfaction level is associated with decrease in probability of turnover.\n",
    "\n",
    "4. Let's practice!\n",
    "01:39 - 01:41\n",
    "Now it's your turn to practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8739b56-36fb-47ab-b948-fa4c526ad672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluir la tabla de correlaciones tal cual está en la imagen dentro de esta carpeta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dae37e-1e14-4973-939d-317a3941d77c",
   "metadata": {},
   "source": [
    "**Percentage of employees who churn**\n",
    "\n",
    "The column churn is providing information about whether an employee has left the company or not is the column churn:\n",
    "\n",
    "if the value of this column is 0, the employee is still with the company\n",
    "if the value of this column is 1, then the employee has left the company\n",
    "Let’s calculate the turnover rate:\n",
    "\n",
    "you will first count the number of times the variable churn has the value 1 and the value 0, respectively\n",
    "you will then divide both counts by the total, and multiply the result by 100 to get the percentage of employees who left and stayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588521a-809e-47c3-bc0a-aa225bce6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use len() function to get the total number of observations and save it as the number of employees\n",
    "n_employees = len(data)\n",
    "\n",
    "print(type(data))\n",
    "print(n_employees)\n",
    "print(data.head(3))\n",
    "print(data.tail(3))\n",
    "\n",
    "# Print the number of employees who left/stayed\n",
    "print(data.churn.value_counts())\n",
    "\n",
    "# Print the percentage of employees who left/stayed\n",
    "print(data.churn.value_counts()/n_employees*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e8890-cecf-4a7f-96c4-cef650ffcc3b",
   "metadata": {},
   "source": [
    "## B. Predicting employee turnover\n",
    "\n",
    "This chapter introduces one of the most popular classification techniques: the Decision Tree. You will use it to develop an algorithm that predicts employee turnover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371213c1-baa8-4bd6-8fcc-e03ed81fe783",
   "metadata": {},
   "source": [
    "### 1. Splitting the data\n",
    "00:00 - 00:09\n",
    "Hello and welcome to Chapter 2. For now we already know how to transform HR data and make it ready for predictive analytics. Let's now concentrate on predictive component.\n",
    "\n",
    "2. Target and features\n",
    "00:09 - 00:38\n",
    "Our target in this course is to predict employee turnover using data that we have on them. In business analytics or data science terminology, the variable that one aims to predict is known as target, while everything else that is used for prediction are called features. In other words, we will be using features to predict target.\n",
    "\n",
    "3. Train/test split\n",
    "00:38 - 01:54\n",
    "To make an accurate prediction and build an algorithm that can be useful in reality, in analytics it is a usual practice to split the data into two components: train and test. Train component is used to conduct calculations, optimizations and develop the algorithm, while the remaining test component is used to validate it. For that reason, once our data is separated into target and features, the next step is to split both of them into train and test component. One of most popular Python libraries, that is widely used by data scientists and business analysts is called sklearn. In sklearn, there is almost always a built-in function for most of the analytics tasks, including train/test splitting. As you can see from the code, the function generates 4 outputs. This happens because we split between train and test both target and features so we end up with train and test components for target, and similarly for features. Last but not least, as you can see the functions takes a test_size argument which is 0.25 in our example. This argument tells sklearn to randomly choose 25% of the data and save it as test, while the rest of 75% will be kept for training. In general, when you have quite a big dataset with millions of observations, around 2-3% for test might be enough. But because our datasets in HR are not usually that big, 25% for test seems to be a good practice.\n",
    "\n",
    "4. Overfitting\n",
    "01:54 - 02:50\n",
    "To understand better the reasoning behind train/test split, let's shortly cover the concept of overfitting. Overfitting is one of the most popular problems in analytics. Our first target is to have an accurate model that can helps us to make accurate predictions and decisions based on them. Yet, a model which is accurate on one data, might not be that much accurate on the other. So our second not less important objective is to achieve a model that is generalizable or in other words, works good not only on our current dataset but also in possible future datasets. Overfitting happens, when the model works well on the dataset it was developed on, but is not useful outside of it. So we split the data into train and test components, develop model on train and then validate it on test to make sure our model was not overfitting the training data.\n",
    "\n",
    "5. Let's practice!\n",
    "02:50 - 02:54\n",
    "We will concentrate more on the concept of overfitting, but until then, let's practice splitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4343bf7-4619-45dd-852a-a18b70c23f63",
   "metadata": {},
   "source": [
    "**Separating Target and Features**\n",
    "\n",
    "In order to make a prediction (in this case, whether an employee would leave or not), one needs to separate the dataset into two components:\n",
    "\n",
    "the dependent variable or target which needs to be predicted\n",
    "the independent variables or features that will be used to make a prediction\n",
    "Your task is to separate the target and features. The target you have here is the employee churn, and features include everything else.\n",
    "\n",
    "Reminder: the dataset has already been modified by encoding categorical variables and getting dummies.\n",
    "\n",
    "pandas has been imported for you as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4e0bc-2493-4064-93c2-5ea7acf731c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target and features\n",
    "\n",
    "# Choose the dependent variable column (churn) and set it as target\n",
    "target = data.churn\n",
    "\n",
    "print(target.head())\n",
    "\n",
    "print(target.value_counts())\n",
    "\n",
    "# Drop column churn and set everything else as features\n",
    "features = data.drop(\"churn\",axis=1)\n",
    "\n",
    "print(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4059f84-aa41-43ea-9d38-d530063b75f2",
   "metadata": {},
   "source": [
    "**Spliting employee data**\n",
    "\n",
    "Overfitting the dataset is a common problem in analytics. This happens when a model is working well on the dataset it was developed upon, but fails to generalize outside of it.\n",
    "\n",
    "A train/test split is implemented to ensure model generalization: you develop the model using the training sample and try it out on the test sample later on.\n",
    "\n",
    "In this exercise, you will split both target and features into train and test sets with 75%/25% ratio, respectively.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import train_test_split from the sklearn.model_selection module\n",
    "Use train_test_split() to split your dataset into training and testing sets\n",
    "Assign 25% of your observations to the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbd808-c8c4-4561-b1c5-ecb3c712ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for splitting dataset into train and test\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Use that function to create the splits both for target and for features\n",
    "# Set the test sample to be 25% of your observations\n",
    "target_train, target_test, features_train, features_test = train_test_split(target,features,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324cd111-a2d8-4285-8d75-909494210c78",
   "metadata": {},
   "source": [
    "### 2. Introduction to Decision Tree classification\n",
    "\n",
    "In this lecture we will start the process of building the predictive algorithm. What we want to accomplish is to have an algorithm, that will learn from our historical data the important variables affecting the decision of leaving the company and use that information to predict turnover. As the values our target, turnover, gets are 2: 1 and 0, this problem is called binary classification.\n",
    "\n",
    "2. Classification in Python\n",
    "00:25 - 00:56\n",
    "There are many different data science/machine learning algorithms that one can use to address binary classification problem such as prediction of employee turnover. Each of them has its own pros and cons and business cases where they are best to apply. The algorithm which we will use showed to be quite popular in HR analytics and is called Decision Tree. The latter is popular for 2 reasons among all: 1st it is able to provide accurate predictions and 2nd, it can be used to understand factors that are driving the decision to leave the company.\n",
    "\n",
    "3. Decision Tree Classification\n",
    "00:56 - 02:13\n",
    "The picture you see know is the visualization of a small sample Decision Tree for employee turnover. The appearance of the algorithm is the reason it is called Decision Tree. Let's go step by step over the tree to understand the classification process. The tree is growing first starting from the variable Satisfaction. It is checked whether for a given employee the satisfaction level was higher than 0.5 or not. If it was, we go to the right branch of the tree, otherwise we move to the left one. If we moved to the right, then the next question we need to ask according to the tree is whether Salary is High or not. As you can see, if Salary is High, then we reach one of the last nodes or leaves of the tree, where the output is that the employee will not Churn. Thus we have a decision path: employees with High Satisfaction level and High Salary do not Churn. Analogically, employees with low satisfaction level who spent, say 3 years with the company do Churn as presented by the last leaf of the leftmost branch of the tree. Therefore, once we have this tree, we can easily predict whether a given employee will churn or not and also understand what are the important variables that drive churn decision.\n",
    "\n",
    "4. Splitting rule\n",
    "02:13 - 02:43\n",
    "Let's now concentrate shortly on the intuition which is used to split the tree. In general Decision Tree algorithm wants to achieve as pure samples in the last leafs as possible. Mathematically, 2 different rules are quite popular to achieve this task: Gini and Entropy. Objective is the same in both cases, we aim to minimize Gini or Entropy, and both will result in purer samples in the last nodes. As theoretically there is no proven dominance between those 2 methods, we will go on using Gini, as it is doing calculations faster.\n",
    "\n",
    "5. Decision Tree splitting: hypothetical example\n",
    "02:43 - 03:27\n",
    "Let's discuss a hypothetical example. Assume we have a dataset of 100 people. Assume also 40 of them are leavers and 60 stayers. So now let's divide them based on satisfaction level being higher than 0.8 or not. If yes, suppose we end up with 50 people on the left branch, all stayers. On the other hand, right branch includes 10 stayers and 40 leavers. As you can see, this hypothetical splittion results in tremendously decreased Gini: from 0.48 to 0 and 0.08 in two branches respectively. As a result, we have purer samples, especially in the left branch, where we have only stayers, which helps us to make more accurate predictions.\n",
    "\n",
    "6. Let's practice!\n",
    "03:27 - 03:31\n",
    "Good, let's new practice the theory before moving to analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6fd24e-4cfc-4840-a97a-2a7d9d15e117",
   "metadata": {},
   "source": [
    "**Computing Gini index**\n",
    "\n",
    "The decision tree algorithm aims to achieve partitions in the terminal nodes that are as pure as possible. The Gini index is one of the methods used to achieve this. It is calculated based on the proportion of samples in each group.\n",
    "\n",
    "Given the number of people who stayed and left respectively, calculate the Gini index for that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef0efc-5ece-4e90-aab6-4cbce15a7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of people who stayed/left\n",
    "stayed = 37\n",
    "left = 1138\n",
    "\n",
    "#sum of stayed and left\n",
    "total = stayed + left\n",
    "\n",
    "#gini index\n",
    "gini = 2*(stayed/total)*(left/total)\n",
    "\n",
    "print(gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f8019-101d-494a-89f2-d630b6b61515",
   "metadata": {},
   "source": [
    "**Splitting the tree**\n",
    "\n",
    "Given the Gini index that would result from splitting by either variable A or B, respectively, decide by which variable the tree should split next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0b18b-a6a0-4fb3-84cf-53d2b290709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini index in case of splitting by variable A or B\n",
    "gini_A = 0.65\n",
    "gini_B = 0.15\n",
    "\n",
    "# check which Gini is lower and use it for spliting\n",
    "if gini_A < gini_B:\n",
    "    print(\"split by A!\")\n",
    "else:\n",
    "    print(\"split by B!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df4601-43a0-4536-9d20-7efc99642e18",
   "metadata": {},
   "source": [
    "### 3. Predicting employee churn using decision trees\n",
    "\n",
    "As for now, we know how Decision tree works in theory. Let's apply this knowledge and use Python to predict employee churn.\n",
    "\n",
    "2. Decision Tree in Python\n",
    "00:07 - 01:44\n",
    "To get the tree, we need to first import the necessary functions and initialize them. For that reason, we will again use already familiar `sklearn` library. Once imported, we need to initialize this long-named function with a more friendly name and also provided a parameter called random_state. This parameter does not really affect the model results, it just ensures that if you run it 2nd time you will still get the same results. As a consequence, it is not important whether it will be = 1, 20 or anything else, what is important is to give the same values if you need to reproduce same results. Once the model is set up, we can go on and use a fit() method on it to fit our features to the target. As you remember, we used train/test split to develop model on train component but then validate on test. This is done to avoid overfitting. For that reason, we use `features_train` and `target_train` components for fitting. Once we run this piece of code, the tree is already calculated and grown. To test out how good this tree is making its prediction we need to use a method called score(), which is calculating the accuracy score of the prediction. Again, because we developed the model based on the training component, we calculate accuracy score on the test component. The score will show how correct prediction is. For example, the score of 0.65 is showing that we made a correct prediction whether an employee will leave or stay based on our tree for 65% of cases. So to get percentages, we just need to multiply the accuracy score by 100.\n",
    "\n",
    "3. Let's practice!\n",
    "01:44 - 01:47\n",
    "OK, Now it's your turn to calculate the accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700a92b-076d-4e68-8cb6-fa6f5b6482b6",
   "metadata": {},
   "source": [
    "**Fitting the tree to employee data**\n",
    "\n",
    "A train/test split provides the opportunity to develop the classifier on the training component and test it on the rest of the dataset. In this exercise, you will start developing an employee turnover prediction model using the decision tree classification algorithm. The algorithm provides a .fit() method, which can be used to fit the features to the model in the training set.\n",
    "\n",
    "Reminder: both target and features are already split into train and test components (Train: features_train, target_train, Test: features_test, target_test)\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the classification algorithm called DecisionTreeClassifier.\n",
    "Initialize it as model and set the random state to 42.\n",
    "Apply the decision tree model by fitting the training set features to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40227e-18b0-4664-b04f-06e3b1778d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the classification algorithm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize it and call model by specifying the random_state parameter\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Apply a decision tree model to fit features to the target\n",
    "model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9feb42e-84ad-4a6c-ba0d-c869e9b0f59a",
   "metadata": {},
   "source": [
    "**Checking the accuracy of prediction**\n",
    "\n",
    "It’s now time to check how well your trained model can make predictions! Let’s use your testing set to check the accuracy of your Decision Tree model, with the score() method.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Apply the decision tree model to fit the features to the target in the training set.\n",
    "Check the accuracy score() of the prediction for the training set.\n",
    "Check the accuracy score() of the prediction for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e04efd-7acf-4b5c-b316-b2b5e2bddac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a decision tree model to fit features to the target in the training set\n",
    "print(model.fit(features_train,target_train))\n",
    "\n",
    "# Check the accuracy score of the prediction for the training set\n",
    "print(model.score(features_train,target_train)*100)\n",
    "\n",
    "# Check the accuracy score of the prediction for the test set\n",
    "print(model.score(features_test,target_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfe20c-0ee9-4a73-989b-2073e75de34c",
   "metadata": {},
   "source": [
    "### 4. Interpretation of the decision tree\n",
    "\n",
    "One of the main advantages of using Decision Tree algorithm, is that it is interpretable. We can visualize the tree to understand the path taking us to the final decision.\n",
    "\n",
    "2. Visualization\n",
    "00:11 - 00:39\n",
    "The visualization consists of 3 steps. First, one needs to export the tree. It will be exported into a file called tree.dot which will reside in the working directory, together with turnover.csv dataset file that we are using. Then you need to open the file and copy content. Last step is going to the webgraphviz website, pasting the content and visualizing the tree.\n",
    "\n",
    "3. Interpretation\n",
    "00:39 - 01:09\n",
    "After the three steps are implemented, you will see similar tree on the webgraphviz website. As you can see each node includes information on sample size, which is number of employees in that leaf who satisfied the proceeding decision rules. It also provides number of stayers and leavers in each node and the corresponding Gini index value. It is visible that once the tree is growing Gini is decreasing, which is the objective we wanted to achieve.\n",
    "\n",
    "4. Let's practice!\n",
    "01:09 - 01:14\n",
    "Now let's try some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8d9cd-3e18-49da-b666-84942303d1a5",
   "metadata": {},
   "source": [
    "**Exporting the tree**\n",
    "\n",
    "In Decision Tree classification tasks, overfitting is usually the result of deeply grown trees. As the comparison of accuracy scores on the train and test sets shows, you have overfitting in your results. This can also be learned from the tree visualization.\n",
    "\n",
    "In this exercise, you will export the decision tree into a text document, which can then be used for visualization.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the the export_graphviz() function from the the sklearn.tree submodule.\n",
    "Fit the model to the training data.\n",
    "Export the visualization to the file tree.dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d5637-fea9-43e2-b873-f5a43f1088c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the graphical visualization export function\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Apply Decision Tree model to fit Features to the Target\n",
    "model.fit(features_train,target_train)\n",
    "\n",
    "# Export the tree to a dot file\n",
    "export_graphviz(model,\"tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a52053-3ef8-4845-becb-b66f9aec30f5",
   "metadata": {},
   "source": [
    "Incluir imagen 'DesicionTreeInterpretation.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407a491-7cef-46f0-93ff-58fa592c6ef9",
   "metadata": {},
   "source": [
    "## C. Evaluating the turnover prediction model\n",
    "\n",
    "Here, you will learn how to evaluate a model and understand how \"good\" it is. You will compare different trees to choose the best among them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a6e083-a417-4189-8c94-6b803c8f8903",
   "metadata": {},
   "source": [
    "### 1. Tuning employee turnover classifier\n",
    "\n",
    "In chapter 2 we shortly touched the concept of overfitting. As it was mentioned there, train/test split helps us to learn whether we have any overfitting error or not. Yet, it does not really provide solution to that. In this chapter, we will concentrate on tuning our classifier to get better results and some of these methods will be related to fighting overfitting.\n",
    "\n",
    "2. Overfitting\n",
    "00:23 - 01:25\n",
    "For that reason, let's remember what Overfitting was about. Once we develop model on the train component, it may work perfectly on that, but fail outside of it. This is the reason we use test component to understand whether our model is useful outside of train data or not. As you can see, the accuracy score is perfect on training set, but not that much high on testing set. This is speaking about overfitting problem. The reason we have it, is because currently, our tree is growing as much as it can grow, and in the end becomes very large and very specific to training data only. To solve this issue, we have two solutions: either we need to limit the maximum depth of the tree, say we do not let the tree to grow more than 5 steps OR we limit the sample size in each leaf and, say, do not allow the tree to grow more if only 100 employees are left in the node/leaf. Let's go on and apply both separately.\n",
    "\n",
    "3. Pruning the tree\n",
    "01:25 - 02:41\n",
    "In the upper block we limit the tree depth. As you can see, this can easily be done by setting an additional parameter `max_depth=5` in the DecisionTreeClassifier during the initialization process. It will help us to keep everything else the same, but limit the tree to at most 5 levels to grow in depth. Thus, let's call this model `model_depth_5`. As you can see, afterwards, the fitting and scoring processes are still the same, with only one tiny but important difference: we fit features to the target and we calculate the accuracy for `model_depth_5` instead of the general model without any limitation. As a result, the accuracy is decreased on both sets, but the difference between them is negligible, which means we reduced overfitting and current model is more realistic. In the lower block we implement everything absolutely the same, apart from the model initialization step again: this time we set `min_sample_leaf=100` to limit the sample size inside a leaf. After fitting and scoring this new model we receive a test accuracy of 96.13% which is again lower, but again, more realistic than the old one.\n",
    "\n",
    "4. Let's practice!\n",
    "02:41 - 02:55\n",
    "We will learn more realistic metrics for evaluating the model. Until then, let's practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8ad9f-e267-44d7-b8c0-ccdf6bd2a643",
   "metadata": {},
   "source": [
    "#### Pruning the tree\n",
    "\n",
    "Overfitting is a classic problem in analytics, especially for the decision tree algorithm. Once the tree is fully grown, it may provide highly accurate predictions for the training sample, yet fail to be that accurate on the test set. For that reason, the growth of the decision tree is usually controlled by:\n",
    "\n",
    "“Pruning” the tree and setting a limit on the maximum depth it can have.\n",
    "Limiting the minimum number of observations in one leaf of the tree.\n",
    "In this exercise, you will:\n",
    "\n",
    "prune the tree and limit the growth of the tree to 5 levels of depth\n",
    "fit it to the employee data\n",
    "test prediction results on both training and testing sets.\n",
    "The variables features_train, target_train, features_test and target_test are already available in your workspace.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Initialize the DecisionTreeClassifier while limiting the depth of the tree to 5.\n",
    "Fit the Decision Tree model using the features and the target in the training set.\n",
    "Check the accuracy of the predictions on both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2dbb26-4d12-4465-9299-77cca82335ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DecisionTreeClassifier while limiting the depth of the tree to 5\n",
    "model_depth_5 = DecisionTreeClassifier(max_depth = 5, random_state = 42)\n",
    "\n",
    "# Fit the model\n",
    "model_depth_5.fit(features_train,target_train)\n",
    "\n",
    "# Print the accuracy of the prediction for the training set\n",
    "print(model_depth_5.score(features_train,target_train) * 100)\n",
    "\n",
    "# Print the accuracy of the prediction for the test set\n",
    "print(model_depth_5.score(features_test,target_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f0d63-fbdb-4586-b641-f8f513378b8f",
   "metadata": {},
   "source": [
    "#### Limiting the sample size\n",
    "\n",
    "Another method to prevent overfitting is to specify the minimum number of observations necessary to grow a leaf (or node), in the Decision Tree.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "set this minimum limit to 100\n",
    "fit the new model to the employee data\n",
    "examine prediction results on both training and test sets\n",
    "The variables features_train, target_train, features_test and target_test are already available in your workspace.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Initialize the DecisionTreeClassifier and set the leaf minimum limit to 100 observations\n",
    "Fit the decision tree model to the training data.\n",
    "Check the accuracy of the predictions on both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1831c16-d051-46eb-9440-8d9d08978b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DecisionTreeClassifier while limiting the sample \n",
    "# size in leaves to 100\n",
    "\n",
    "model_sample_100 = DecisionTreeClassifier(  min_samples_leaf = 100, \n",
    "                                            random_state = 42)\n",
    "\n",
    "# Fit the model\n",
    "model_sample_100.fit(features_train,target_train)\n",
    "\n",
    "# Print the accuracy of the prediction (in percentage points) for the training set\n",
    "print(model_sample_100.score(features_train,target_train) * 100)\n",
    "\n",
    "# Print the accuracy of the prediction (in percentage points) for the test set\n",
    "print(model_sample_100.score(features_test,target_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b989b-43ce-45fe-9e1a-f8de40610017",
   "metadata": {},
   "source": [
    "### 2. Evaluating the model\n",
    "00:00 - 00:17\n",
    "Before now, we were using only general accuracy score to evaluate the performance of our model. However, it turns out only accuracy is not enough to claim that the model is a good one.\n",
    "\n",
    "2. Prediction errors\n",
    "00:17 - 00:55\n",
    "To understand what other metrics of evaluation are doing, let me introduce you to prediction errors first. We have two possible outcomes in reality which means in general we have 4 possible situations presented in this so called confusion matrix. When the prediction is 0, we call it negative, and when it is 1, it is widely accepted to call it positive. Similarly, when prediction is correct, we say it is True, otherwise it is False. Thus, if in reality someone left the company but was predicted to be a stayer, then we have False Negative, as the prediction was both False and Negative. Based on this 4 possibilities, many different metrics are developed in analytics to measure performance of the model.\n",
    "\n",
    "3. Evaluation metrics (1)\n",
    "00:55 - 01:29\n",
    "If the target of your predictions is mostly to focus on those who are churning, then you probably want to have less False Negatives, people who leave in reality but your algorithm is not able to predict it. For that reason, Recall score can be useful. Higher values of recall correspond lower values of False Negatives. One the other hand, if you want to keep your attention on those who stay, less False Positives will be your target, which can be achieved with higher Specificity score.\n",
    "\n",
    "4. Evaluation metrics (2)\n",
    "01:29 - 02:17\n",
    "There are some other metrics that can be derived from the same confusion matrix. For example, if one is interested in learning what is the percentage of people who truly left the company among those who were predicted to leave, then Precision score will be handy to use. The reason those scores are important is that general accuracy is not providing information about separate classes. For example, in our model around 76% are stayers. So if we just say \"everybody is staying\" we will have 76% accurate prediction. But in terms of recall, we will have very low value, as everybody who churned will be wrongly classified.\n",
    "\n",
    "5. Let's practice!\n",
    "02:17 - 02:23\n",
    "My experience shows that sometimes those scores sound very similar and are difficult to differentiate in between. If you feel so, do not worry, and take your time to go over confusion matrix again to understand the intuition behind each of them. As for now let's calculate some measures for our employee dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb5825-7f3f-48ea-9fa2-b8ebc167d0b7",
   "metadata": {},
   "source": [
    "#### Calculating accuracy metrics: precision\n",
    "\n",
    "The Precision score is an important metric used to measure the accuracy of a classification algorithm. It is calculated as the fraction of True Positives over the sum of True Positives and False Positives, or\n",
    "\n",
    "$$\n",
    "`# of True Positives / (# of True Positives + # of False Positives)`\n",
    "$$\n",
    "\n",
    "we define True Positives as the number of employees who actually left, and were classified correctly as leaving\n",
    "we define False Positives as the number of employees who actually stayed, but were wrongly classified as leaving\n",
    "If there are no False Positives, the precision score is equal to 1. If there are no True Positives, the precision score is equal to 0.\n",
    "\n",
    "In this exercise, we will calculate the precision score (using the sklearn function precision_score) for our initial classification model.\n",
    "\n",
    "The variables features_test and target_test are available in your workspace.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the function precision_score from the module sklearn.metrics.\n",
    "Use the initial model to predict churn (based on features of the test set).\n",
    "Calculate the precision score by comparing target_test with the test set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51efea1-6488-416b-b4b0-9c59023dd76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to calculate precision score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Predict whether employees will churn using the test set\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "print(prediction.size)\n",
    "print(prediction.shape)\n",
    "print(prediction.mean)\n",
    "print(prediction)\n",
    "\n",
    "# Calculate precision score by comparing target_test with the prediction\n",
    "precision_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43178892-79ac-4f10-ad4b-87d245e8e45e",
   "metadata": {},
   "source": [
    "#### Calculating accuracy metrics: recall\n",
    "The Recall score is another important metric used to measure the accuracy of a classification algorithm. It is calculated as the** fraction of True Positives over the sum of True Positives and False Negatives**, or\n",
    "\n",
    "$$\n",
    "`# of True Positives / (# of True Positives + # of False Negatives)`\n",
    "$$\n",
    "\n",
    "If there are no False Negatives, the recall score is equal to 1. If there are no True Positives, the recall score is equal to 0.\n",
    "\n",
    "In this exercise, you will calculate the recall score (using the sklearn function recall_score) for your initial classification model.\n",
    "\n",
    "The variables features_test and target_test are available in your workspace.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the function to calculate the recall score.\n",
    "Use the initial model to predict churn (based on features of the test set).\n",
    "Calculate the recall score by comparing target_test with the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f4777-b973-4cef-b895-817c1e6fa008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to calculate recall score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Use the initial model to predict churn\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "# Calculate recall score by comparing \n",
    "# target_test with the prediction\n",
    "recall_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd9615-cece-4144-a75e-d0d8059a3cce",
   "metadata": {},
   "source": [
    "### 3. Targeting both leavers and stayers\n",
    "\n",
    "As the objective of this course, is to develop a model that will correctly predict churn, recall score seems to be our target. However, recall alone is not enough, as by only targeting one class, we may have dramatically low accuracy for the other. Thus, a general rule is to use a measure that is not concentrated on one class alone.\n",
    "\n",
    "2. AUC score\n",
    "00:20 - 01:14\n",
    "If our target are leavers, we would concentrate on recall, if stayers, then on specificity. But if your target is to have good predictions on both, then probably the best choice is to use AUC score. AUC stands for Area Under Curve and is basically a compound measure that is maximized when both recall and specificity are maximized. To calculate AUC score, one needs to place Recall on vertical, and 1- Specificity on horizontal axis and draw the Blue curve in the graph, which is called ROC. The are between ROC that we obtained and the green diagonal line that a random prediction could obtain is the AUC score Using AUC as a target to maximize, the model will try to correctly classify both 1s and 0s keeping an eye on recall and specificity at the same time.\n",
    "\n",
    "3. Let's practice!\n",
    "01:14 - 01:19\n",
    "Excellent, let's now put this into practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd5ba6-d750-4bf7-9e8f-0bdcaf2136c6",
   "metadata": {},
   "source": [
    "#### Calculating the ROC/AUC score\n",
    "\n",
    "While the Recall score is an important metric for measuring the accuracy of a classification algorithm, it puts too much weight on the number of False Negatives. On the other hand, Precision is concentrated on the number of False Positives.\n",
    "\n",
    "The combination of those two results in the ROC curve allows us to measure both recall and precision. The area under the ROC curve is calculated as the AUC score.\n",
    "\n",
    "In this exercise, you will calculate the ROC/AUC score for the initial model using the sklearn roc_auc_score() function.\n",
    "\n",
    "The variables features_test and target_test are available in your workspace.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the function to calculate ROC/AUC score.\n",
    "Use the initial model to predict churn (based on the features of the test set).\n",
    "Calculate ROC/AUC score by comparing target_test with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40eff6-d8ac-4f88-ae5a-24781366467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to calculate ROC/AUC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Use initial model to predict churn \n",
    "# (based on features_test)\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "# Calculate ROC/AUC score by comparing \n",
    "# target_test with the prediction\n",
    "roc_auc_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbcce5-0066-41cd-b590-652a362af08b",
   "metadata": {},
   "source": [
    "### 4. Class imbalance\n",
    "\n",
    "General accuracy score is a good choice only if classes in the dataset are balanced. However, as discussed in this chapter, class imbalance may lead to higher accuracy score, when in fact our model is failing to correctly predict churn. This was the reason we covered evaluation metrics other than accuracy score. While those other metrics are more robust and informative, they only partially solve the class imbalance problem. To solve it, what we can do is to change prior probabilities.\n",
    "\n",
    "2. Prior probabilities\n",
    "00:28 - 01:33\n",
    "As you remember, Gini index was the objective of our Decision tree to minimize and it was calculated based on probability of being 1 or 0. As we have no other information about probabilities, in the very beginning, when the tree just starts to grow, in order to calculate the Gini index, it takes proportions of 0s and 1s as probabilities in Gini formula. As a result, Class 0, which are stayers, becomes more influential as they are 76% of the observations in our dataset. This is the reason, our algorithm was able to correctly predict 0s but not 1s. To solve it, we just need to tell Python to balance class weights which will make probability of both being 0 and 1 equal to 50%. This will probably negatively affect the general accuracy as a result of increased Gini, but AUC and especially Recall should probably be improved, as now both classes are equally important.\n",
    "\n",
    "3. Let's practice!\n",
    "01:33 - 01:39\n",
    "Let's now implement this change and see what happens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3065b-b603-49b5-b9e6-4c3b9f8f6130",
   "metadata": {},
   "source": [
    "#### Balancing classes\n",
    "\n",
    "It can significantly affect prediction results, as shown by the difference between the recall and accuracy scores. To solve the imbalance, equal weights are usually given to each class. Using the class_weight argument in sklearn's DecisionTreeClassifier, one can make the classes become \"balanced\".\n",
    "\n",
    "Let’s correct our model by solving its imbalance problem:\n",
    "\n",
    "first, you’re going to set up a model with balanced classes\n",
    "then, you will fit it to the training data\n",
    "finally, you will check its accuracy on the test set\n",
    "The variables features_train, target_train, features_test and target_test are already available in your workspace.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Initialize the Decision Tree Classifier, prune your tree by limiting its maximum depth to 5, and balance the class weights.\n",
    "Fit the new model.\n",
    "Print the accuracy score of the prediction (in percentage points) for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178a7ff-0dd7-4149-9f16-4c97de8373e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DecisionTreeClassifier \n",
    "model_depth_5_b = DecisionTreeClassifier( max_depth=5,\n",
    "                            class_weight=\"balanced\",\n",
    "                            random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model_depth_5_b.fit(features_train,target_train)\n",
    "\n",
    "# Print the accuracy of the prediction \n",
    "# (in percentage points) for the test set\n",
    "print(model_depth_5_b.score(features_test,target_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2185ac-2435-43cb-aa1b-2855be69b676",
   "metadata": {},
   "source": [
    "#### Comparison of Employee attrition models\n",
    "\n",
    "In this exercise, your task is to compare the balanced and imbalanced (default) models using the pruned tree (max_depth=7). The imbalanced model is already done using recall and ROC/AUC scores. Complete the same steps for the balanced model.\n",
    "\n",
    "The variables features_train, target_train, features_test and target_test are already available in your workspace.\n",
    "An imbalanced model has already been fit for you and, and its predictions saved as prediction.\n",
    "The functions recall_score() and roc_auc_score() have been imported for you.\n",
    "Instructions\n",
    "100 XP\n",
    "Initialize the balanced model, setting its maximum depth to 7, and its seed to 42.\n",
    "Fit it to the training component using the training set.\n",
    "Make predictions using the testing set.\n",
    "Print the recall score and ROC/AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5332db1-6690-46c1-8076-d59dddcc1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the recall score\n",
    "print(recall_score(target_test,prediction))\n",
    "# Print the ROC/AUC score\n",
    "print(roc_auc_score(target_test,prediction))\n",
    "\n",
    "# Initialize the model\n",
    "model_depth_7_b = DecisionTreeClassifier(max_depth = 7, \n",
    "                    class_weight = 'balanced',\n",
    "                    random_state = 42)\n",
    "\n",
    "# Fit it to the training component\n",
    "model_depth_7_b.fit(features_train, target_train)\n",
    "\n",
    "# Make prediction using test component\n",
    "prediction_b = model_depth_7_b.predict(features_test)\n",
    "\n",
    "# Print the recall score for the balanced model\n",
    "print(recall_score(target_test, prediction_b))\n",
    "\n",
    "# Print the ROC/AUC score for the balanced model\n",
    "print(roc_auc_score(target_test, prediction_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b171dab6-b369-4069-a0f8-4616630a4cbd",
   "metadata": {},
   "source": [
    "## D. Choosing the best turnover prediction model\n",
    "\n",
    "In this final chapter, you will learn how to use cross-validation to avoid overfitting the training data. You will also learn how to know which features are impactful, and which are negligible. Finally, you will use these newly acquired skills to build a better performing Decision Tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6c633-92a1-443b-a76a-a436169182f3",
   "metadata": {},
   "source": [
    "### 1. Hyperparameter tuning\n",
    "\n",
    "Welcome to the final Chapter of the course. Congratulations on making this far. Now, we are already familiar with the key approaches to tune and evaluate our model. However, one question that you may have been asking until now is how we decide whether, for example, maximum depth of the tree should be set to 5 or 6 or 10 or any other value. Same goes for other parameters covered until now. The answer is very simple, we just try different values and find the one that provides best possible predictions.\n",
    "\n",
    "2. GridSearch\n",
    "00:48 - 02:03\n",
    "Maximum depth, minimum sample size and similar other parameters that need to be tuned to find the best value are known as hyperparameters. To find the optimal values for those hyperparameters, one needs to create a grid, a list of applicable values that he or she wants to test and then search among those values the one that achieves highest accuracy. For example, the maximum depth should not attain very high values, as the tree will start to overfit, but low values are not acceptable as well, as they may provide biased and less accurate predictions. For that reason, let's try to find the optimal value between 5 and 20. Similarly, for minimum sample size in the leaf nodes, let's check values between 50 and 450 with a step of 50. Once those values are generated inside a list, the only thing left is to develop Decision Tree for all possible combinations of those values and compare them to find the values that provide best performance on the test set. This process is known as GridSearch and while it may sound confusing, implementation in Python using sklearn is fairly easy.\n",
    "\n",
    "3. Cross-Validation\n",
    "02:03 - 02:36\n",
    "While Train/test split ensures that the model does not overfit training component, hyperparameter tuning may result in overfitting the test component. As a solution, one is encouraged to validate the model on different test components, which is achieved using **Cross Validation**. The latter is general case of Train/test split, as it splits the data into **k** components or folds, where each component has the opportunity of being the **test component**. In this example picture, we have 5 folds, and during each one of the components is Test while others are used as Train. Having different Folds, ensures that our model does not overfit the test component. This is exactly what GridSearch in sklearn is using to understand which model is better.\n",
    "\n",
    "4. Let's practice!\n",
    "02:36 - 02:42\n",
    "Now let's try some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f0cba-fef9-4c9a-9fad-cc579bd0cfd2",
   "metadata": {},
   "source": [
    "#### Cross-validation using sklearn\n",
    "\n",
    "As explained in Chapter 2, overfitting the dataset is a common problem in analytics. This happens when a model has learned the data too closely: it has great performances on the dataset it was trained on, but fails to generalize outside of it.\n",
    "\n",
    "While the train/test split technique you learned in Chapter 2 ensures that the model does not overfit the training set, hyperparameter tuning may result in overfitting the test component, since it consists in tuning the model to get the best prediction results on the test set. Therefore, it is recommended to validate the model on different testing sets. K-fold cross-validation allows us to achieve this:\n",
    "\n",
    "- it splits the dataset into a training set and a testing set\n",
    "- it fits the model, makes predictions and calculates a score (you can specify if you want the accuracy, precision, recall…)\n",
    "- it repeats the process k times in total\n",
    "- it outputs the average of the 10 scores\n",
    "\n",
    "In this exercise, you will use Cross Validation on our dataset, and evaluate our results with the cross_val_score function.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the function for implementing cross-validation, cross_val_score(), from the module sklearn.model_selection.\n",
    "Print the cross-validation score for your model, specifying 10 folds with the cv hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f59a4-474c-4985-967d-45352cefb090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for implementing cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use that function to print the cross validation score for 10 folds\n",
    "print(cross_val_score(model,features,target,cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1444bc9-b6b0-414e-a9bb-1286f2fd679f",
   "metadata": {},
   "source": [
    "#### Setting up GridSearch parameters\n",
    "\n",
    "A hyperparameter is a parameter inside a function. For example, max_depth or min_samples_leaf are hyperparameters of the DecisionTreeClassifier() function. Hyperparameter tuning is the process of testing different values of hyperparameters to find the optimal ones: the one that gives the best predictions according to your objectives. In sklearn, you can use GridSearch to test different combinations of hyperparameters. Even better, you can use GridSearchCV() to test different combinations and run cross-validation on them in one function!\n",
    "\n",
    "In this exercise, you are going to prepare the different values you want to test for max_depth and min_samples_leaf. You will then put these in a dictionary, because that’s what is required for GridSearchCV():\n",
    "\n",
    "the dictionary keys will be the hyperparameters names\n",
    "the dictionary values will be the attributes (the hyperparameter values) you want to test\n",
    "Instead of writing all the values manually, you will use the range() function, which allows us to generate values incrementally. For example, range(1, 10, 2) will generate a list containing values ranging from 1 included to 10 not included, by increments of 2. So the final result will be [1, 3, 5, 7, 9].\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Following the format in the example above, generate values for the maximum depth ranging from 5 to 20 with increments of 1\n",
    "Do the same for the minimum sample size with values from 50 to 450 with increments of 50\n",
    "Create the dictionary by specifying the max_depth and min_samples_leaf values to try, respective values, using the variables you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf687c-fecd-44c5-8ce5-ddb5bae0c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values for maximum depth\n",
    "depth = [i for i in range(5,21,1)]\n",
    "\n",
    "# Generate values for minimum sample size\n",
    "samples = [i for i in range(50,500,50)]\n",
    "\n",
    "# Create the dictionary with parameters to be checked\n",
    "parameters = dict(max_depth=depth, min_samples_leaf=samples)\n",
    "\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1be5a3-1daf-42f4-a0d4-15f3a016b208",
   "metadata": {},
   "source": [
    "#### Implementing GridSearch\n",
    "\n",
    "You can now use the sklearn GridSearchCV() function to find the best combination of all of the max_depth and min_samples_leaf values you generated in the previous exercise.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the GridSearchCV function\n",
    "Apply a GridSearchCV() function to your model using the parameters dictionary you defined earlier. Save this as param_search.\n",
    "Fit param_search to the training dataset.\n",
    "Print the best parameters found using best_params_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6251fb-8647-4fd8-a205-5246e3e735d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the GridSearchCV function\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# set up parameters: done\n",
    "parameters = dict(max_depth=depth, min_samples_leaf=samples)\n",
    "\n",
    "# initialize the param_search function using the GridSearchCV function, initial model and parameters above\n",
    "param_search = GridSearchCV(model, parameters)\n",
    "\n",
    "# fit the param_search to the training dataset\n",
    "param_search.fit(features_train, target_train)\n",
    "\n",
    "# print the best parameters found\n",
    "print(param_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bda24-7cdb-4ff0-897b-455437d6abff",
   "metadata": {},
   "source": [
    "### 2. Important features for predicting attrition\n",
    "00:00 - 00:14\n",
    "One of the main reasons we chose to use Decision Tree algorithm is that it provides interpretability. We can not only visualize and explain it, but we can also understand what are the important features that drive the decision to leave the company.\n",
    "\n",
    "2. Feature Importances\n",
    "00:14 - 00:50\n",
    "Fortunately, once Decision Tree is developed, sklearn can easily calculate feature importances. The latter is equal to the relative decrease in Gini due to the selected feature. Once the calculation is done for all features, the values are rescaled to sum up to 100%. As a result, higher percentage speaks about the feature being more important. Usually, results show that not all the features are that important. As a consequence, if you learn that a feature is not important at all, it is suggested to drop it and run the model without that feature.\n",
    "\n",
    "3. Let's practice!\n",
    "00:50 - 00:53\n",
    "Let's now find the important features in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3cd4b6-aa71-4fe9-a335-ae352592d291",
   "metadata": {},
   "source": [
    "#### Sorting important features\n",
    "\n",
    "Among other things, Decision Trees are very popular because of their interpretability. Many models can provide accurate predictions, but Decision Trees can also quantify the effect of the different features on the target. Here, it can tell you which features have the strongest and weakest impacts on the decision to leave the company. In sklearn, you can get this information by using the feature_importances_ attribute.\n",
    "\n",
    "In this exercise, you're going to get the quantified importance of each feature, save them in a pandas DataFrame (a Pythonic table), and sort them from the most important to the less important. The model_ best Decision Tree Classifier used in the previous exercises is available in your workspace, as well as the features_test and features_train variables.\n",
    "\n",
    "pandas has been imported as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use the feature_importances_ attribute to calculate relative feature importances\n",
    "Create a list of features\n",
    "Save the results inside a DataFrame using the DataFrame() function, where the features are rows and their respective values are a column\n",
    "Sort the relative_importances DataFrame to get the most important features on top using the sort_values() function and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f6994-b99b-454a-af47-91fbf0e54309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importances using the \n",
    "# 'feature_importances_' attribute\n",
    "feature_importances = model_best.feature_importances_\n",
    "\n",
    "# Create a list of features: done\n",
    "feature_list = list(features)\n",
    "\n",
    "# Save the results inside a DataFrame using feature_list \n",
    "# as an index the features are the rows\n",
    "relative_importances = pd.DataFrame(index = feature_list, \n",
    "                            data=feature_importances, \n",
    "                            columns=[\"importance\"])\n",
    "\n",
    "# Sort values to learn most important features\n",
    "relative_importances.sort_values(by=\"importance\", \n",
    "                            ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85796176-2056-4753-969d-24bd425b48f7",
   "metadata": {},
   "source": [
    "#### Selecting important features\n",
    "In this exercise, your task is to select only the most important features that will be used by the final model. Remember, that the relative importances are saved in the column importance of the DataFrame called relative_importances.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Select only the features with an importance value higher than 1%.\n",
    "Create a list from those features and print them (this has been done for you).\n",
    "Using the index saved in selected_list, transform both features_train and features_test to include the features with an importance higher than 1% only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403eabec-987d-4ace-a93c-8cc7d16e88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECTING IMPORTANT FEATURES\n",
    "##############################\n",
    "# select only features with relative importance higher than 1%\n",
    "selected_features = relative_importances [relative_importances.values > 0.01]\n",
    "\n",
    "# create a list from those features: done\n",
    "selected_list = selected_features.index\n",
    "\n",
    "# transform both features_train and features_test components to include only selected features\n",
    "features_train_selected = features_train[selected_list]\n",
    "features_test_selected = features_test[selected_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e36d74-69f5-44d0-ad97-ee18dd1800e1",
   "metadata": {},
   "source": [
    "#### Develop and test the best model\n",
    "In Chapter 3, you found out that the following parameters allow you to get better model:\n",
    "\n",
    "max_depth = 8,\n",
    "min_samples_leaf = 150,\n",
    "class_weight = \"balanced\"\n",
    "In this chapter, you discovered that some of the features have a negligible impact. You realized that you could get accurate predictions using just a small number of selected, impactful features and you updated your training and testing set accordingly, creating the variables features_train_selected and features_test_selected.\n",
    "\n",
    "With all this information at your disposal, you're now going to develop the best model for predicting employee turnover and evaluate it using the appropriate metrics.\n",
    "\n",
    "The features_train_selected and features_test_selected variables are available in your workspace, and the recall_score and roc_auc_score functions have been imported for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Initialize the best model using the parameters provided in the description.\n",
    "Fit the model using only the selected features from the training set.\n",
    "Make a prediction based on the selected features from the test set.\n",
    "Print the accuracy, recall and ROC/AUC scores of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2ce73-1bf0-4e9f-8074-9de75e01ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop and test the best model\n",
    "# Initialize the best model using parameters provided \n",
    "# in description\n",
    "model_best = DecisionTreeClassifier(max_depth=8,\n",
    "                min_samples_leaf = 150,\n",
    "                class_weight = 'balanced', \n",
    "                random_state=42)\n",
    "\n",
    "# Fit the model using only selected features from training set: done\n",
    "model_best.fit(features_train_selected, target_train)\n",
    "\n",
    "# Make prediction based on selected list of features from test set\n",
    "prediction_best = model_best.predict(features_test_selected)\n",
    "\n",
    "# Print the general accuracy of the model_best\n",
    "print(model_best.score(features_test_selected, target_test) * 100)\n",
    "\n",
    "# Print the recall score of the model predictions\n",
    "print(recall_score(target_test, prediction_best) * 100)\n",
    "\n",
    "# Print the ROC/AUC score of the model predictions\n",
    "print(roc_auc_score(target_test, prediction_best) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca1c276-5883-4a71-b15c-0febf5063975",
   "metadata": {},
   "source": [
    "### 3. Final thoughts\n",
    "00:00 - 00:13\n",
    "Congratulations! Now you have mastered predicting churn using Decision Trees. One thing that I would like to note here, is that although we concentrated on this model, Decision Trees are not the only choice to predict employee turnover.\n",
    "\n",
    "2. Alternative methods\n",
    "00:13 - 01:18\n",
    "One very popular alternative that is used widely in HR analytics is Logistic Regression. You may still use Python and sklearn to make predictions with Logistic Regression using the evaluation metrics and approaches discussed in this course. While single Decision Tree is good, sometimes many is better. The tree based algorithms like Random Forest or Gradient Boosting usually provide better results than a single Decision Tree. The reason we use single one, is that those complex models are not interpretable and cannot be visualized to make decisions using Decision path. Last but not list, Neural networks are popular alternative nowadays for many prediction tasks including turnover prediction. However, they are considered a black box and do not provide the clue behind their predictions, which especially in HR is very important. As you have completed this course, now my advice would be to take some other HR datasets and attach your predictive skills on them following the same order of tasks that we have completed during this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a4e0c-667d-4389-81ac-cf8a04b9ba49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
